setwd("c:/r/itcs")
# Set the working directory 

library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("tau")
# Read required libraries 

d = read.csv("dsat.csv", stringsAsFactors=FALSE)
# Read data from csv file

d = d[,3]
# Read only comments column


myCorpus <- Corpus(VectorSource(d))
# Creates the corpose vector 

inspect(myCorpus[1])

myCorpus <- tm_map(myCorpus, tolower)
# COnverts to lower case, punctioations, remove number, stemdocument, remove stop words, remov white space
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus,stemDocument)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus<- tm_map(myCorpus, PlainTextDocument)


dtm <- TermDocumentMatrix(myCorpus)
# Constructs or coerces to a term-document matrix or a document-term matrix.
m <- as.matrix(dtm)

##### Trying for N Gram##############

tdm.1g <- TermDocumentMatrix(myCorpus)
dtm.1g <- DocumentTermMatrix(myCorpus)

### Find the most frequent terms
findFreqTerms(tdm.1g, 40)
## Find frequent terms in a document-term or term-document matrix.
## Playing to see terms with different frequencies
findFreqTerms(tdm.1g, 60)
findFreqTerms(tdm.1g, 80)
findFreqTerms(tdm.1g, 100)

findAssocs(dtm.1g, "issue", .25)
findAssocs(dtm.1g, "time", .25)
findAssocs(dtm.1g, "problem", .25)
findAssocs(dtm.1g, "help", .25)
findAssocs(dtm.1g, "ticket", .25)
findAssocs(dtm.1g, "call", .25)
## Did not find any interesting corelations ##

tdm89.1g <- removeSparseTerms(tdm.1g, 0.89)
tdm9.1g  <- removeSparseTerms(tdm.1g, 0.9)
tdm91.1g <- removeSparseTerms(tdm.1g, 0.91)
tdm92.1g <- removeSparseTerms(tdm.1g, 0.92)

tdm2.1g <- tdm92.1g

# Creates a Boolean matrix (counts # docs w/terms, not raw # terms)
tdm3.1g <- inspect(tdm2.1g)

tdm3.1g[tdm3.1g>=1] <- 1 

# Transform into a term-term adjacency matrix

termMatrix.1gram <- tdm3.1g %*% t(tdm3.1g)

# inspect terms numbered 5 to 10
termMatrix.1gram[5:10,5:10]

termMatrix.1gram[1:10,1:10]

# Create a WordCloud to Visualize the Text Data ---------------------------
notsparse <- tdm2.1g

m = as.matrix(notsparse)

v = sort(rowSums(m),decreasing=TRUE)

d = data.frame(word = names(v),freq=v)

# Create the word cloud
pal = brewer.pal(9,"BuPu")
wordcloud(words = d$word,
          freq = d$freq,
          scale = c(3,.8),
          random.order = F,
          colors = pal)

tokenize_ngrams <- function(x, n=3) return(rownames(as.data.frame(unclass(textcnt(x,method="string",n=n)))))

# BigramTokenizer ####

setwd("c:/r/itcs")
# Set the working directory 

library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("RWeka")

# Read required libraries 

d = read.csv("dsat.csv", stringsAsFactors=FALSE)
# Read data from csv file

d = d[,3]
# Read only comments column


myCorpus <- Corpus(VectorSource(d))
# Creates the corpose vector 

inspect(myCorpus[1])

myCorpus <- tm_map(myCorpus, tolower)
# COnverts to lower case, punctioations, remove number, stemdocument, remove stop words, remov white space
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus,stemDocument)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus<- tm_map(myCorpus, PlainTextDocument)


# BigramTokenizer ####
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))


tdm.ng <- TermDocumentMatrix(myCorpus, control = list(tokenize = BigramTokenizer))
dtm.ng <- DocumentTermMatrix(myCorpus, control = list(tokenize = BigramTokenizer))

# Try removing sparse terms at a few different levels but didn't gave any results after sparsity removal so kept it
tdm89.ng <- removeSparseTerms(tdm.ng, 0.89)
tdm9.ng  <- removeSparseTerms(tdm.ng, 0.9)
tdm91.ng <- removeSparseTerms(tdm.ng, 0.91)
tdm92.ng <- removeSparseTerms(tdm.ng, 0.92)
tdm10.ng <- removeSparseTerms(tdm.ng, 0.10)

notsparse <- tdm.ng
inspect(notsparse)

m = as.matrix(notsparse)
v = sort(rowSums(m),decreasing=TRUE)
d = data.frame(word = names(v),freq=v)
d

# Create the word cloud

wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"),ordered.colors=FALSE)

# Create bar chart of 20 top word pairs

barplot(width = 2,d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word,
        col ="lightblue", main ="20 Most frequent word pairs",
        ylab = "Word frequencies")

## Same Steps for Dsat

d = read.csv("sat.csv", stringsAsFactors=FALSE)
# Read data from csv file

d = d[,3]
# Read only comments column


myCorpus <- Corpus(VectorSource(d))
# Creates the corpose vector 

inspect(myCorpus[1])

myCorpus <- tm_map(myCorpus, tolower)
# COnverts to lower case, punctioations, remove number, stemdocument, remove stop words, remov white space
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus,stemDocument)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus<- tm_map(myCorpus, PlainTextDocument)


# BigramTokenizer ####
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))


tdm.ng <- TermDocumentMatrix(myCorpus, control = list(tokenize = BigramTokenizer))
dtm.ng <- DocumentTermMatrix(myCorpus, control = list(tokenize = BigramTokenizer))


notsparse <- tdm.ng
inspect(notsparse)

m = as.matrix(notsparse)
v = sort(rowSums(m),decreasing=TRUE)
d = data.frame(word = names(v),freq=v)
d

# Create the word cloud

wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"),ordered.colors=FALSE)

# Create bar chart of 20 top word pairs

barplot(width = 2,d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word,
        col ="lightblue", main ="20 Most frequent word pairs",
        ylab = "Word frequencies")


